{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Report on Movie Review Classification\n",
        "Github Link: https://github.com/soobino/adv_machine_learning\n",
        "\n",
        "*   QMSS 5074\n",
        "*   Advanced Machine Learning \n",
        "*  Spring 2023"
      ],
      "metadata": {
        "id": "D5su6nQ0ACTE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set Up"
      ],
      "metadata": {
        "id": "UYEK0tdbBrKx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Discuss the dataset in general terms and describe why building a predictive model using this data might be practically useful.  Who could benefit from a model like this? Explain.**"
      ],
      "metadata": {
        "id": "7Ydl67PY-teG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#install aimodelshare library\n",
        "! pip install aimodelshare==0.0.189"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GSQmcTE0AOwg",
        "outputId": "150c3b68-4586-47d4-b698-6fc36884f1f0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting aimodelshare==0.0.189\n",
            "  Downloading aimodelshare-0.0.189-py3-none-any.whl (967 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m967.8/967.8 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting scikit-learn==1.2.1\n",
            "  Downloading scikit_learn-1.2.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.6/9.6 MB\u001b[0m \u001b[31m59.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.9/dist-packages (from aimodelshare==0.0.189) (2022.10.31)\n",
            "Collecting tensorflow==2.9.2\n",
            "  Downloading tensorflow-2.9.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (511.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m511.8/511.8 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting Pympler==0.9\n",
            "  Downloading Pympler-0.9.tar.gz (178 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m178.4/178.4 kB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch>=1.8.1 in /usr/local/lib/python3.9/dist-packages (from aimodelshare==0.0.189) (2.0.0+cu118)\n",
            "Collecting boto3==1.26.69\n",
            "  Downloading boto3-1.26.69-py3-none-any.whl (132 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.7/132.7 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tf2onnx\n",
            "  Downloading tf2onnx-1.14.0-py3-none-any.whl (451 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m451.2/451.2 kB\u001b[0m \u001b[31m24.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting PyJWT>=2.4.0\n",
            "  Downloading PyJWT-2.6.0-py3-none-any.whl (20 kB)\n",
            "Collecting protobuf==3.19.6\n",
            "  Downloading protobuf-3.19.6-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m45.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.9/dist-packages (from aimodelshare==0.0.189) (1.6.3)\n",
            "Collecting wget==3.2\n",
            "  Downloading wget-3.2.zip (10 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting onnx==1.12.0\n",
            "  Downloading onnx-1.12.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m39.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting importlib-resources==5.10.0\n",
            "  Downloading importlib_resources-5.10.0-py3-none-any.whl (34 kB)\n",
            "Collecting scipy==1.7.0\n",
            "  Downloading scipy-1.7.0-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.whl (28.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m28.4/28.4 MB\u001b[0m \u001b[31m32.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting onnxconverter-common>=1.7.0\n",
            "  Downloading onnxconverter_common-1.13.0-py2.py3-none-any.whl (83 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.8/83.8 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: psutil>=5.9.1 in /usr/local/lib/python3.9/dist-packages (from aimodelshare==0.0.189) (5.9.4)\n",
            "Collecting shortuuid>=1.0.8\n",
            "  Downloading shortuuid-1.0.11-py3-none-any.whl (10 kB)\n",
            "Collecting docker==5.0.0\n",
            "  Downloading docker-5.0.0-py2.py3-none-any.whl (146 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.0/147.0 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting onnxruntime>=1.7.0\n",
            "  Downloading onnxruntime-1.14.1-cp39-cp39-manylinux_2_27_x86_64.whl (5.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.0/5.0 MB\u001b[0m \u001b[31m50.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pydot==1.3.0\n",
            "  Downloading pydot-1.3.0-py2.py3-none-any.whl (18 kB)\n",
            "Collecting onnxmltools>=1.6.1\n",
            "  Downloading onnxmltools-1.11.2-py2.py3-none-any.whl (322 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.5/322.5 kB\u001b[0m \u001b[31m27.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: seaborn>=0.11.2 in /usr/local/lib/python3.9/dist-packages (from aimodelshare==0.0.189) (0.12.2)\n",
            "Collecting skl2onnx>=1.14.0\n",
            "  Downloading skl2onnx-1.14.0-py2.py3-none-any.whl (294 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.0/294.0 kB\u001b[0m \u001b[31m28.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pathlib>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from aimodelshare==0.0.189) (1.0.1)\n",
            "Collecting keras2onnx>=1.7.0\n",
            "  Downloading keras2onnx-1.7.0-py3-none-any.whl (96 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.3/96.3 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting botocore==1.29.82\n",
            "  Downloading botocore-1.29.82-py3-none-any.whl (10.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.5/10.5 MB\u001b[0m \u001b[31m64.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.9/dist-packages (from astunparse==1.6.3->aimodelshare==0.0.189) (0.40.0)\n",
            "Requirement already satisfied: six<2.0,>=1.6.1 in /usr/local/lib/python3.9/dist-packages (from astunparse==1.6.3->aimodelshare==0.0.189) (1.16.0)\n",
            "Collecting jmespath<2.0.0,>=0.7.1\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Collecting s3transfer<0.7.0,>=0.6.0\n",
            "  Downloading s3transfer-0.6.0-py3-none-any.whl (79 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.6/79.6 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: urllib3<1.27,>=1.25.4 in /usr/local/lib/python3.9/dist-packages (from botocore==1.29.82->aimodelshare==0.0.189) (1.26.15)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.9/dist-packages (from botocore==1.29.82->aimodelshare==0.0.189) (2.8.2)\n",
            "Requirement already satisfied: requests!=2.18.0,>=2.14.2 in /usr/local/lib/python3.9/dist-packages (from docker==5.0.0->aimodelshare==0.0.189) (2.27.1)\n",
            "Requirement already satisfied: websocket-client>=0.32.0 in /usr/local/lib/python3.9/dist-packages (from docker==5.0.0->aimodelshare==0.0.189) (1.5.1)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.9/dist-packages (from importlib-resources==5.10.0->aimodelshare==0.0.189) (3.15.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.2.1 in /usr/local/lib/python3.9/dist-packages (from onnx==1.12.0->aimodelshare==0.0.189) (4.5.0)\n",
            "Requirement already satisfied: numpy>=1.16.6 in /usr/local/lib/python3.9/dist-packages (from onnx==1.12.0->aimodelshare==0.0.189) (1.22.4)\n",
            "Requirement already satisfied: pyparsing>=2.1.4 in /usr/local/lib/python3.9/dist-packages (from pydot==1.3.0->aimodelshare==0.0.189) (3.0.9)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn==1.2.1->aimodelshare==0.0.189) (3.1.0)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from scikit-learn==1.2.1->aimodelshare==0.0.189) (1.2.0)\n",
            "Collecting flatbuffers<2,>=1.12\n",
            "  Downloading flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.9.2->aimodelshare==0.0.189) (0.2.0)\n",
            "Collecting tensorboard<2.10,>=2.9\n",
            "  Downloading tensorboard-2.9.1-py3-none-any.whl (5.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m75.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.9.2->aimodelshare==0.0.189) (16.0.0)\n",
            "Collecting tensorflow-estimator<2.10.0,>=2.9.0rc0\n",
            "  Downloading tensorflow_estimator-2.9.0-py2.py3-none-any.whl (438 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m438.7/438.7 kB\u001b[0m \u001b[31m37.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.9.2->aimodelshare==0.0.189) (67.6.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.9.2->aimodelshare==0.0.189) (1.53.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.9.2->aimodelshare==0.0.189) (3.8.0)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.9.2->aimodelshare==0.0.189) (0.4.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.9.2->aimodelshare==0.0.189) (0.32.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.9.2->aimodelshare==0.0.189) (3.3.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.9.2->aimodelshare==0.0.189) (2.2.0)\n",
            "Collecting keras<2.10.0,>=2.9.0rc0\n",
            "  Downloading keras-2.9.0-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.9.2->aimodelshare==0.0.189) (23.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.9.2->aimodelshare==0.0.189) (1.4.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.9.2->aimodelshare==0.0.189) (1.14.1)\n",
            "Collecting keras-preprocessing>=1.1.1\n",
            "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fire\n",
            "  Downloading fire-0.5.0.tar.gz (88 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.3/88.3 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.9/dist-packages (from onnxruntime>=1.7.0->aimodelshare==0.0.189) (1.11.1)\n",
            "Collecting coloredlogs\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas>=0.25 in /usr/local/lib/python3.9/dist-packages (from seaborn>=0.11.2->aimodelshare==0.0.189) (1.5.3)\n",
            "Requirement already satisfied: matplotlib!=3.6.1,>=3.1 in /usr/local/lib/python3.9/dist-packages (from seaborn>=0.11.2->aimodelshare==0.0.189) (3.7.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from torch>=1.8.1->aimodelshare==0.0.189) (3.1.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from torch>=1.8.1->aimodelshare==0.0.189) (3.11.0)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.9/dist-packages (from torch>=1.8.1->aimodelshare==0.0.189) (2.0.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.9/dist-packages (from torch>=1.8.1->aimodelshare==0.0.189) (3.1)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch>=1.8.1->aimodelshare==0.0.189) (16.0.1)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch>=1.8.1->aimodelshare==0.0.189) (3.25.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib!=3.6.1,>=3.1->seaborn>=0.11.2->aimodelshare==0.0.189) (8.4.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib!=3.6.1,>=3.1->seaborn>=0.11.2->aimodelshare==0.0.189) (4.39.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib!=3.6.1,>=3.1->seaborn>=0.11.2->aimodelshare==0.0.189) (1.0.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib!=3.6.1,>=3.1->seaborn>=0.11.2->aimodelshare==0.0.189) (1.4.4)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.9/dist-packages (from matplotlib!=3.6.1,>=3.1->seaborn>=0.11.2->aimodelshare==0.0.189) (0.11.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas>=0.25->seaborn>=0.11.2->aimodelshare==0.0.189) (2022.7.1)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests!=2.18.0,>=2.14.2->docker==5.0.0->aimodelshare==0.0.189) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests!=2.18.0,>=2.14.2->docker==5.0.0->aimodelshare==0.0.189) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests!=2.18.0,>=2.14.2->docker==5.0.0->aimodelshare==0.0.189) (3.4)\n",
            "Collecting tensorboard-data-server<0.7.0,>=0.6.0\n",
            "  Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m45.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9.2->aimodelshare==0.0.189) (2.17.2)\n",
            "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
            "  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9.2->aimodelshare==0.0.189) (3.4.3)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9.2->aimodelshare==0.0.189) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9.2->aimodelshare==0.0.189) (2.2.3)\n",
            "Collecting humanfriendly>=9.1\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->torch>=1.8.1->aimodelshare==0.0.189) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.9/dist-packages (from sympy->onnxruntime>=1.7.0->aimodelshare==0.0.189) (1.3.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow==2.9.2->aimodelshare==0.0.189) (0.2.8)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow==2.9.2->aimodelshare==0.0.189) (5.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow==2.9.2->aimodelshare==0.0.189) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow==2.9.2->aimodelshare==0.0.189) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.9/dist-packages (from markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow==2.9.2->aimodelshare==0.0.189) (6.3.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.9/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow==2.9.2->aimodelshare==0.0.189) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.9/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow==2.9.2->aimodelshare==0.0.189) (3.2.2)\n",
            "Building wheels for collected packages: Pympler, wget, fire\n",
            "  Building wheel for Pympler (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for Pympler: filename=Pympler-0.9-py3-none-any.whl size=164822 sha256=cbeb78b4a782a68264f9f4582566b084cba846fdac88eec29f5fce63467cd436\n",
            "  Stored in directory: /root/.cache/pip/wheels/03/84/9a/0e7aa69b52bc9dc13ea25679c8d8f1dc11bc5b5289db23d9f3\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9676 sha256=5806e8befe2e8b2efd4e2530af809baddab3eff100067075145cc3728c8840a5\n",
            "  Stored in directory: /root/.cache/pip/wheels/04/5f/3e/46cc37c5d698415694d83f607f833f83f0149e49b3af9d0f38\n",
            "  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fire: filename=fire-0.5.0-py2.py3-none-any.whl size=116952 sha256=14a1b940b12398a230d1c56e6c2c87fd3b79276acd255fe39230270858584d33\n",
            "  Stored in directory: /root/.cache/pip/wheels/f7/f1/89/b9ea2bf8f80ec027a88fef1d354b3816b4d3d29530988972f6\n",
            "Successfully built Pympler wget fire\n",
            "Installing collected packages: wget, Pympler, keras, flatbuffers, tensorflow-estimator, tensorboard-data-server, shortuuid, scipy, PyJWT, pydot, protobuf, keras-preprocessing, jmespath, importlib-resources, humanfriendly, fire, scikit-learn, onnx, docker, coloredlogs, botocore, tf2onnx, s3transfer, onnxruntime, onnxconverter-common, google-auth-oauthlib, tensorboard, skl2onnx, keras2onnx, boto3, tensorflow, onnxmltools, aimodelshare\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.12.0\n",
            "    Uninstalling keras-2.12.0:\n",
            "      Successfully uninstalled keras-2.12.0\n",
            "  Attempting uninstall: flatbuffers\n",
            "    Found existing installation: flatbuffers 23.3.3\n",
            "    Uninstalling flatbuffers-23.3.3:\n",
            "      Successfully uninstalled flatbuffers-23.3.3\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.12.0\n",
            "    Uninstalling tensorflow-estimator-2.12.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.12.0\n",
            "  Attempting uninstall: tensorboard-data-server\n",
            "    Found existing installation: tensorboard-data-server 0.7.0\n",
            "    Uninstalling tensorboard-data-server-0.7.0:\n",
            "      Successfully uninstalled tensorboard-data-server-0.7.0\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.10.1\n",
            "    Uninstalling scipy-1.10.1:\n",
            "      Successfully uninstalled scipy-1.10.1\n",
            "  Attempting uninstall: pydot\n",
            "    Found existing installation: pydot 1.4.2\n",
            "    Uninstalling pydot-1.4.2:\n",
            "      Successfully uninstalled pydot-1.4.2\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 3.20.3\n",
            "    Uninstalling protobuf-3.20.3:\n",
            "      Successfully uninstalled protobuf-3.20.3\n",
            "  Attempting uninstall: importlib-resources\n",
            "    Found existing installation: importlib-resources 5.12.0\n",
            "    Uninstalling importlib-resources-5.12.0:\n",
            "      Successfully uninstalled importlib-resources-5.12.0\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.2.2\n",
            "    Uninstalling scikit-learn-1.2.2:\n",
            "      Successfully uninstalled scikit-learn-1.2.2\n",
            "  Attempting uninstall: google-auth-oauthlib\n",
            "    Found existing installation: google-auth-oauthlib 1.0.0\n",
            "    Uninstalling google-auth-oauthlib-1.0.0:\n",
            "      Successfully uninstalled google-auth-oauthlib-1.0.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.12.1\n",
            "    Uninstalling tensorboard-2.12.1:\n",
            "      Successfully uninstalled tensorboard-2.12.1\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.12.0\n",
            "    Uninstalling tensorflow-2.12.0:\n",
            "      Successfully uninstalled tensorflow-2.12.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "arviz 0.15.1 requires scipy>=1.8.0, but you have scipy 1.7.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed PyJWT-2.6.0 Pympler-0.9 aimodelshare-0.0.189 boto3-1.26.69 botocore-1.29.82 coloredlogs-15.0.1 docker-5.0.0 fire-0.5.0 flatbuffers-1.12 google-auth-oauthlib-0.4.6 humanfriendly-10.0 importlib-resources-5.10.0 jmespath-1.0.1 keras-2.9.0 keras-preprocessing-1.1.2 keras2onnx-1.7.0 onnx-1.12.0 onnxconverter-common-1.13.0 onnxmltools-1.11.2 onnxruntime-1.14.1 protobuf-3.19.6 pydot-1.3.0 s3transfer-0.6.0 scikit-learn-1.2.1 scipy-1.7.0 shortuuid-1.0.11 skl2onnx-1.14.0 tensorboard-2.9.1 tensorboard-data-server-0.6.1 tensorflow-2.9.2 tensorflow-estimator-2.9.0 tf2onnx-1.14.0 wget-3.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get competition data\n",
        "from aimodelshare import download_data\n",
        "download_data('public.ecr.aws/y2e2a1d6/sst2_competition_data-repository:latest') "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fmjhcV1lAPri",
        "outputId": "0f17a17c-7e89-4fff-b012-09b9a5db24c9"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading [=============================================>   ]\n",
            "\n",
            "Data downloaded successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up X_train, X_test, and y_train_labels objects\n",
        "import pandas as pd\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=Warning)\n",
        "\n",
        "X_train=pd.read_csv(\"sst2_competition_data/X_train.csv\", squeeze=True)\n",
        "X_test=pd.read_csv(\"sst2_competition_data/X_test.csv\", squeeze=True)\n",
        "\n",
        "y_train_labels=pd.read_csv(\"sst2_competition_data/y_train_labels.csv\", squeeze=True)\n",
        "\n",
        "y_train = pd.get_dummies(y_train_labels)"
      ],
      "metadata": {
        "id": "I0TCQzxTCL5F"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.utils import pad_sequences\n",
        "import numpy as np\n",
        "\n",
        "max_words =20000\n",
        "maxlen=100\n",
        "embedding_dim = 100 \n",
        "\n",
        "tokenizer = Tokenizer(num_words=maxlen)\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "\n",
        "def preprocessor(data, maxlen=maxlen, max_words=max_words):\n",
        "\n",
        "    sequences = tokenizer.texts_to_sequences(data)\n",
        "\n",
        "    word_index = tokenizer.word_index\n",
        "    X = pad_sequences(sequences, maxlen=maxlen)\n",
        "\n",
        "    return X"
      ],
      "metadata": {
        "id": "jZy62U9VCQVO"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import aimodelshare as ai\n",
        "ai.export_preprocessor(preprocessor,\"\") "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Grce0STD1qk",
        "outputId": "a5eb80d1-b865-4f68-d051-29c21d383e3d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your preprocessor is now saved to 'preprocessor.zip'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Set credentials using modelshare.org username/password\n",
        "\n",
        "from aimodelshare.aws import set_credentials\n",
        "    \n",
        "apiurl=\"https://rlxjxnoql9.execute-api.us-east-1.amazonaws.com/prod/m\" #This is the unique rest api that powers this specific Playground\n",
        "\n",
        "set_credentials(apiurl=apiurl)\n",
        "\n",
        "mycompetition= ai.Competition(apiurl)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y82yZwpDDm_4",
        "outputId": "aa927e3f-36ce-4de0-860d-953d0c19ecc3"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AI Modelshare Username:··········\n",
            "AI Modelshare Password:··········\n",
            "AI Model Share login credentials set successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prediction Models Part 1"
      ],
      "metadata": {
        "id": "mZYgyxRkBujX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**1. Use an Embedding layer and LSTM layers**"
      ],
      "metadata": {
        "id": "QKMkDo8y-vm7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Embedding, LSTM, Flatten\n",
        "\n",
        "model_1 = Sequential()\n",
        "model_1.add(Embedding(max_words, embedding_dim, input_length=maxlen))\n",
        "model_1.add(LSTM(64, return_sequences=True, dropout=0.2))\n",
        "model_1.add(LSTM(32, dropout=0.2))\n",
        "model_1.add(Flatten())\n",
        "model_1.add(Dense(2, activation='softmax'))\n",
        "\n",
        "model_1.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3OQKEFg1B3Ih",
        "outputId": "f1ace591-f2fb-4f22-e724-9694a921ca3d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_8\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_8 (Embedding)     (None, 100, 100)          2000000   \n",
            "                                                                 \n",
            " lstm_8 (LSTM)               (None, 100, 64)           42240     \n",
            "                                                                 \n",
            " lstm_9 (LSTM)               (None, 32)                12416     \n",
            "                                                                 \n",
            " flatten_5 (Flatten)         (None, 32)                0         \n",
            "                                                                 \n",
            " dense_11 (Dense)            (None, 2)                 66        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,054,722\n",
            "Trainable params: 2,054,722\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_1.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
        "history = model_1.fit(preprocessor(X_train), y_train,\n",
        "                    epochs=5,\n",
        "                    batch_size=26,\n",
        "                    validation_split=0.2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P7ZxVhNUe-UI",
        "outputId": "c7b4f196-55da-4e2b-edb5-7e92b89b953f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "213/213 [==============================] - 39s 163ms/step - loss: 0.6508 - acc: 0.6281 - val_loss: 0.8193 - val_acc: 0.3945\n",
            "Epoch 2/5\n",
            "213/213 [==============================] - 34s 158ms/step - loss: 0.6183 - acc: 0.6669 - val_loss: 0.8943 - val_acc: 0.3584\n",
            "Epoch 3/5\n",
            "213/213 [==============================] - 32s 148ms/step - loss: 0.6104 - acc: 0.6709 - val_loss: 0.8085 - val_acc: 0.4227\n",
            "Epoch 4/5\n",
            "213/213 [==============================] - 33s 156ms/step - loss: 0.6026 - acc: 0.6810 - val_loss: 0.8284 - val_acc: 0.3873\n",
            "Epoch 5/5\n",
            "213/213 [==============================] - 34s 159ms/step - loss: 0.6004 - acc: 0.6808 - val_loss: 0.8922 - val_acc: 0.3728\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from aimodelshare.aimsonnx import model_to_onnx\n",
        "\n",
        "onnx_model = model_to_onnx(model_1, framework='keras',\n",
        "                          transfer_learning=False,\n",
        "                          deep_learning=True)\n",
        "\n",
        "with open(\"model_1.onnx\", \"wb\") as f:\n",
        "    f.write(onnx_model.SerializeToString())"
      ],
      "metadata": {
        "id": "fI6e9ef6C_L6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prediction_column_index=model_1.predict(preprocessor(X_test)).argmax(axis=1)\n",
        "prediction_labels = [y_train.columns[i] for i in prediction_column_index]\n",
        "mycompetition.submit_model(model_filepath = \"model_1.onnx\",\n",
        "                                 preprocessor_filepath=\"preprocessor.zip\",\n",
        "                                 prediction_submission=prediction_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "upYAKzDcDHZX",
        "outputId": "6b842a84-1c5d-4eb5-c617-8c51c8543c5e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "57/57 [==============================] - 3s 31ms/step\n",
            "Insert search tags to help users find your model (optional): \n",
            "Provide any useful notes about your model (optional): \n",
            "\n",
            "Your model has been submitted as model version 176\n",
            "\n",
            "To submit code used to create this model or to view current leaderboard navigate to Model Playground: \n",
            "\n",
            " https://www.modelshare.org/detail/model:2763\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**2. Use an Embedding layer and Conv1d layers**"
      ],
      "metadata": {
        "id": "bhtFAKQjDYyG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from tensorflow.keras.layers import SimpleRNN, LSTM,Embedding\n",
        "\n",
        "model_2 = Sequential()\n",
        "model_2.add(layers.Embedding(max_words, embedding_dim, input_length=maxlen))\n",
        "model_2.add(layers.Conv1D(64, 7, activation='relu')) \n",
        "model_2.add(layers.Conv1D(32, 7, activation='relu')) \n",
        "model_2.add(layers.Conv1D(16, 7, activation='relu')) \n",
        "model_2.add(layers.MaxPooling1D(5)) #\n",
        "model_2.add(layers.GlobalMaxPooling1D())\n",
        "model_2.add(layers.Dense(2, activation='softmax'))\n",
        "\n",
        "model_2.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1lGCVkibE6vM",
        "outputId": "25b53eec-bee5-48d0-c6d2-669fd2c3eec2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_9\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_9 (Embedding)     (None, 100, 100)          2000000   \n",
            "                                                                 \n",
            " conv1d_9 (Conv1D)           (None, 94, 64)            44864     \n",
            "                                                                 \n",
            " conv1d_10 (Conv1D)          (None, 88, 32)            14368     \n",
            "                                                                 \n",
            " conv1d_11 (Conv1D)          (None, 82, 16)            3600      \n",
            "                                                                 \n",
            " max_pooling1d_3 (MaxPooling  (None, 16, 16)           0         \n",
            " 1D)                                                             \n",
            "                                                                 \n",
            " global_max_pooling1d_3 (Glo  (None, 16)               0         \n",
            " balMaxPooling1D)                                                \n",
            "                                                                 \n",
            " dense_12 (Dense)            (None, 2)                 34        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,062,866\n",
            "Trainable params: 2,062,866\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_2.compile(optimizer=RMSprop(lr=1e-4),\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['acc'])\n",
        "history = model_2.fit(preprocessor(X_train), y_train,\n",
        "                    epochs=5,\n",
        "                    batch_size=26,\n",
        "                    validation_split=0.2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_oZc6nA9OreT",
        "outputId": "8b94798d-2d76-44be-b4bc-97173295a173"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "213/213 [==============================] - 14s 60ms/step - loss: 0.6728 - acc: 0.6133 - val_loss: 0.8718 - val_acc: 0.1488\n",
            "Epoch 2/5\n",
            "213/213 [==============================] - 9s 42ms/step - loss: 0.6666 - acc: 0.6149 - val_loss: 0.8965 - val_acc: 0.1488\n",
            "Epoch 3/5\n",
            "213/213 [==============================] - 12s 56ms/step - loss: 0.6654 - acc: 0.6149 - val_loss: 0.9534 - val_acc: 0.1488\n",
            "Epoch 4/5\n",
            "213/213 [==============================] - 10s 47ms/step - loss: 0.6647 - acc: 0.6149 - val_loss: 0.8720 - val_acc: 0.1488\n",
            "Epoch 5/5\n",
            "213/213 [==============================] - 11s 50ms/step - loss: 0.6627 - acc: 0.6149 - val_loss: 0.9249 - val_acc: 0.1488\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from aimodelshare.aimsonnx import model_to_onnx\n",
        "\n",
        "onnx_model = model_to_onnx(model_2, framework='keras',\n",
        "                          transfer_learning=False,\n",
        "                          deep_learning=True)\n",
        "\n",
        "with open(\"model_2.onnx\", \"wb\") as f:\n",
        "    f.write(onnx_model.SerializeToString())"
      ],
      "metadata": {
        "id": "NAXdhkWIUsg-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prediction_column_index=model_2.predict(preprocessor(X_test)).argmax(axis=1)\n",
        "prediction_labels = [y_train.columns[i] for i in prediction_column_index]\n",
        "mycompetition.submit_model(model_filepath = \"model_2.onnx\",\n",
        "                                 preprocessor_filepath=\"preprocessor.zip\",\n",
        "                                 prediction_submission=prediction_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gFtwLpRFUw9U",
        "outputId": "1110dd46-64db-437a-f542-95a5288edb3e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "57/57 [==============================] - 1s 11ms/step\n",
            "Insert search tags to help users find your model (optional): \n",
            "Provide any useful notes about your model (optional): \n",
            "\n",
            "Your model has been submitted as model version 177\n",
            "\n",
            "To submit code used to create this model or to view current leaderboard navigate to Model Playground: \n",
            "\n",
            " https://www.modelshare.org/detail/model:2763\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**3. Use transfer learning with glove embeddings**"
      ],
      "metadata": {
        "id": "c1gRbzzPDeF2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download Glove embedding matrix weights\n",
        "! wget http://nlp.stanford.edu/data/wordvecs/glove.6B.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rmmLnTYMFCjE",
        "outputId": "d5aa2c50-043c-4a81-971c-9206c5ad75e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-04-15 19:45:34--  http://nlp.stanford.edu/data/wordvecs/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/wordvecs/glove.6B.zip [following]\n",
            "--2023-04-15 19:45:34--  https://nlp.stanford.edu/data/wordvecs/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://downloads.cs.stanford.edu/nlp/data/wordvecs/glove.6B.zip [following]\n",
            "--2023-04-15 19:45:34--  https://downloads.cs.stanford.edu/nlp/data/wordvecs/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182753 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip.2’\n",
            "\n",
            "glove.6B.zip.2      100%[===================>] 822.24M  5.01MB/s    in 2m 39s  \n",
            "\n",
            "2023-04-15 19:48:13 (5.18 MB/s) - ‘glove.6B.zip.2’ saved [862182753/862182753]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! unzip glove.6B.zip "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YtlsW54QHK3L",
        "outputId": "e34f68bd-52e6-4833-adac-71b20b8c4e0a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  glove.6B.zip\n",
            "replace glove.6B.100d.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: N\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "glove_dir = os.getcwd()\n",
        "\n",
        "embeddings_index = {}\n",
        "f = open(os.path.join(glove_dir, 'glove.6B.100d.txt'))\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    embeddings_index[word] = coefs\n",
        "f.close()\n",
        "\n",
        "print('Found %s word vectors.' % len(embeddings_index))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cHeKVl78HOOc",
        "outputId": "730fc459-bf0d-407f-b259-3c6f3d97aab5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 400001 word vectors.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from tensorflow.keras.layers import SimpleRNN, LSTM,Embedding\n",
        "\n",
        "model_3 = tf.keras.Sequential()\n",
        "model_3.add(layers.Embedding(maxlen, embedding_dim, input_length=maxlen))\n",
        "model_3.add(layers.Flatten())\n",
        "model_3.add(layers.Dense(64, activation='relu'))\n",
        "model_3.add(layers.Dense(32, activation='relu'))\n",
        "model_3.add(layers.Dense(16, activation='relu'))\n",
        "model_3.add(layers.Dense(2, activation='softmax'))\n",
        "model_3.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z7rHxw9ZKtAA",
        "outputId": "373bae00-058d-4114-bfff-2a1f79366769"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_12\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_12 (Embedding)    (None, 100, 100)          10000     \n",
            "                                                                 \n",
            " flatten_8 (Flatten)         (None, 10000)             0         \n",
            "                                                                 \n",
            " dense_21 (Dense)            (None, 64)                640064    \n",
            "                                                                 \n",
            " dense_22 (Dense)            (None, 32)                2080      \n",
            "                                                                 \n",
            " dense_23 (Dense)            (None, 16)                528       \n",
            "                                                                 \n",
            " dense_24 (Dense)            (None, 2)                 34        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 652,706\n",
            "Trainable params: 652,706\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "print('Found %s unique tokens.' % len(word_index))\n",
        "\n",
        "embedding_matrix = np.zeros((maxlen, maxlen))\n",
        "for word, i in word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if i < maxlen:\n",
        "        if embedding_vector is not None:\n",
        "            embedding_matrix[i] = embedding_vector"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UcTlbLMSHVqK",
        "outputId": "7cab346a-f8c6-42f0-b76e-5fe83a003b13"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 13835 unique tokens.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_3.layers[0].set_weights([embedding_matrix])\n",
        "model_3.layers[0].trainable = False\n",
        "\n",
        "model_3.compile(optimizer=RMSprop(lr=1e-4),\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['acc'])\n",
        "\n",
        "history = model_3.fit(preprocessor(X_train), y_train,\n",
        "                    epochs=5,\n",
        "                    batch_size=26,\n",
        "                    validation_split=0.2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OzkNGFQRHmJl",
        "outputId": "65be66de-5332-4862-a30a-b73ea820c4a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "213/213 [==============================] - 4s 12ms/step - loss: 0.6685 - acc: 0.6122 - val_loss: 0.8168 - val_acc: 0.1496\n",
            "Epoch 2/5\n",
            "213/213 [==============================] - 2s 10ms/step - loss: 0.6523 - acc: 0.6216 - val_loss: 0.8599 - val_acc: 0.2038\n",
            "Epoch 3/5\n",
            "213/213 [==============================] - 2s 10ms/step - loss: 0.6398 - acc: 0.6317 - val_loss: 0.8720 - val_acc: 0.2442\n",
            "Epoch 4/5\n",
            "213/213 [==============================] - 2s 9ms/step - loss: 0.6279 - acc: 0.6472 - val_loss: 0.8664 - val_acc: 0.2934\n",
            "Epoch 5/5\n",
            "213/213 [==============================] - 2s 10ms/step - loss: 0.6171 - acc: 0.6637 - val_loss: 0.8961 - val_acc: 0.3071\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from aimodelshare.aimsonnx import model_to_onnx\n",
        "\n",
        "onnx_model = model_to_onnx(model_3, framework='keras',\n",
        "                          transfer_learning=True,\n",
        "                          deep_learning=True)\n",
        "\n",
        "with open(\"model_3.onnx\", \"wb\") as f:\n",
        "    f.write(onnx_model.SerializeToString())"
      ],
      "metadata": {
        "id": "M5IbEBBAIP0c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prediction_column_index=model_3.predict(preprocessor(X_test)).argmax(axis=1)\n",
        "prediction_labels = [y_train.columns[i] for i in prediction_column_index]\n",
        "mycompetition.submit_model(model_filepath = \"model_3.onnx\",\n",
        "                                 preprocessor_filepath=\"preprocessor.zip\",\n",
        "                                 prediction_submission=prediction_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IYoDo9HEH9Vw",
        "outputId": "8619b7ed-94aa-4ed8-f34f-bf5325e3a1ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "57/57 [==============================] - 0s 3ms/step\n",
            "Insert search tags to help users find your model (optional): \n",
            "Provide any useful notes about your model (optional): \n",
            "\n",
            "Your model has been submitted as model version 178\n",
            "\n",
            "To submit code used to create this model or to view current leaderboard navigate to Model Playground: \n",
            "\n",
            " https://www.modelshare.org/detail/model:2763\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Discuss which models performed better and point out relevant hyper-parameter values for successful models.\n",
        "Submit your best three models to the leader board for the SST Model Share competition.**"
      ],
      "metadata": {
        "id": "SGLayzDY-5Yk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I experimented 3 models.\n",
        "\n",
        "All of them were fitted with: epochs=5, batch_size=26,validation_split=0.2\n",
        "Last layer was always softmax to classify into two categories: negative and positive sentiment. \n",
        "\n",
        "1. Embedding layer and LSTM layers\n",
        "* 2 LSTM layers\n",
        "* Accuracy of 0.6808 at epoch 5/5\n",
        "\n",
        "2. Embedding layer and Conv1d layers\n",
        "* 3 Conv1d Layers\n",
        "* Accuracy of 0.6149 at epoch 5/5\n",
        "\n",
        "3. Transfer learning with glove embeddings\n",
        "* Accuracy of 0.6637 at epoch 5/5\n",
        "\n",
        "Accuracy was the highest with LSTM layers"
      ],
      "metadata": {
        "id": "YhLu6Bv5jvKx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prediction Models Part 2"
      ],
      "metadata": {
        "id": "t0Uhqhdvl0_7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**After you submit your first three models, describe your best model with your team via your team slack channel. Fit and submit up to three more models after learning from your team.** "
      ],
      "metadata": {
        "id": "761twTDf-7SK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Embedding, LSTM, Flatten\n",
        "\n",
        "model_4 = Sequential()\n",
        "model_4.add(Embedding(max_words, embedding_dim, input_length=maxlen))\n",
        "model_4.add(LSTM(128, return_sequences=True, dropout=0.2))\n",
        "model_4.add(LSTM(128, dropout=0.2))\n",
        "model_4.add(Flatten())\n",
        "model_4.add(Dense(2, activation='softmax'))\n",
        "\n",
        "model_4.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CQWRDhyaFfiv",
        "outputId": "945f53d4-624e-439c-df5d-15bdaf3dbf02"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 100, 100)          2000000   \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, 100, 128)          117248    \n",
            "                                                                 \n",
            " lstm_1 (LSTM)               (None, 128)               131584    \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 128)               0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 2)                 258       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,249,090\n",
            "Trainable params: 2,249,090\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_4.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
        "history = model_4.fit(preprocessor(X_train), y_train,\n",
        "                    epochs=10,\n",
        "                    batch_size=10,\n",
        "                    validation_split=0.2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2JAt51agGCds",
        "outputId": "916bc5c9-e488-4433-8660-20d0468ea767"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "554/554 [==============================] - 182s 315ms/step - loss: 0.6453 - acc: 0.6360 - val_loss: 0.7402 - val_acc: 0.5065\n",
            "Epoch 2/10\n",
            "554/554 [==============================] - 168s 303ms/step - loss: 0.6196 - acc: 0.6671 - val_loss: 0.7719 - val_acc: 0.4559\n",
            "Epoch 3/10\n",
            "554/554 [==============================] - 218s 393ms/step - loss: 0.6116 - acc: 0.6738 - val_loss: 0.9411 - val_acc: 0.3266\n",
            "Epoch 4/10\n",
            "554/554 [==============================] - 167s 301ms/step - loss: 0.6021 - acc: 0.6792 - val_loss: 0.8222 - val_acc: 0.3801\n",
            "Epoch 5/10\n",
            "554/554 [==============================] - 170s 307ms/step - loss: 0.5966 - acc: 0.6828 - val_loss: 0.8490 - val_acc: 0.4704\n",
            "Epoch 6/10\n",
            "554/554 [==============================] - 174s 314ms/step - loss: 0.5883 - acc: 0.6917 - val_loss: 0.8254 - val_acc: 0.4306\n",
            "Epoch 7/10\n",
            "554/554 [==============================] - 160s 288ms/step - loss: 0.5823 - acc: 0.6956 - val_loss: 0.9320 - val_acc: 0.3952\n",
            "Epoch 8/10\n",
            "554/554 [==============================] - 164s 297ms/step - loss: 0.5782 - acc: 0.7007 - val_loss: 0.7974 - val_acc: 0.4538\n",
            "Epoch 9/10\n",
            "554/554 [==============================] - 164s 296ms/step - loss: 0.5712 - acc: 0.7092 - val_loss: 0.8604 - val_acc: 0.4516\n",
            "Epoch 10/10\n",
            "554/554 [==============================] - 160s 289ms/step - loss: 0.5639 - acc: 0.7103 - val_loss: 0.8939 - val_acc: 0.4451\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from aimodelshare.aimsonnx import model_to_onnx\n",
        "\n",
        "onnx_model = model_to_onnx(model_4, framework='keras',\n",
        "                          transfer_learning=False,\n",
        "                          deep_learning=True)\n",
        "\n",
        "with open(\"model_4.onnx\", \"wb\") as f:\n",
        "    f.write(onnx_model.SerializeToString())"
      ],
      "metadata": {
        "id": "PDWRpaM9GHst"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prediction_column_index=model_4.predict(preprocessor(X_test)).argmax(axis=1)\n",
        "prediction_labels = [y_train.columns[i] for i in prediction_column_index]\n",
        "mycompetition.submit_model(model_filepath = \"model_4.onnx\",\n",
        "                                 preprocessor_filepath=\"preprocessor.zip\",\n",
        "                                 prediction_submission=prediction_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NeIxrO81GP4Y",
        "outputId": "1cffa467-3965-4e71-b522-e336f9c9e155"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "57/57 [==============================] - 9s 122ms/step\n",
            "Insert search tags to help users find your model (optional): \n",
            "Provide any useful notes about your model (optional): \n",
            "\n",
            "Your model has been submitted as model version 353\n",
            "\n",
            "To submit code used to create this model or to view current leaderboard navigate to Model Playground: \n",
            "\n",
            " https://www.modelshare.org/detail/model:2763\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Embedding, LSTM, Flatten\n",
        "\n",
        "model_5 = Sequential()\n",
        "model_5.add(Embedding(max_words, embedding_dim, input_length=maxlen))\n",
        "model_5.add(LSTM(64, return_sequences=True, dropout=0.2))\n",
        "model_5.add(LSTM(64, dropout=0.2))\n",
        "model_5.add(Flatten())\n",
        "model_5.add(Dense(2, activation='softmax'))\n",
        "\n",
        "model_5.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ck5J9CH2I634",
        "outputId": "b6f6a414-e927-4a61-854d-0174fb96e43d"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_1 (Embedding)     (None, 100, 100)          2000000   \n",
            "                                                                 \n",
            " lstm_2 (LSTM)               (None, 100, 64)           42240     \n",
            "                                                                 \n",
            " lstm_3 (LSTM)               (None, 64)                33024     \n",
            "                                                                 \n",
            " flatten_1 (Flatten)         (None, 64)                0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 2)                 130       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,075,394\n",
            "Trainable params: 2,075,394\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_5.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
        "history = model_5.fit(preprocessor(X_train), y_train,\n",
        "                    epochs=10,\n",
        "                    batch_size=10,\n",
        "                    validation_split=0.2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AFmYsYn9JEfx",
        "outputId": "eb537023-758d-4555-8de6-c6721685a5a4"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "554/554 [==============================] - 93s 156ms/step - loss: 0.6458 - acc: 0.6395 - val_loss: 0.8745 - val_acc: 0.3548\n",
            "Epoch 2/10\n",
            "554/554 [==============================] - 89s 160ms/step - loss: 0.6172 - acc: 0.6693 - val_loss: 0.8198 - val_acc: 0.4032\n",
            "Epoch 3/10\n",
            "554/554 [==============================] - 87s 157ms/step - loss: 0.6085 - acc: 0.6786 - val_loss: 0.7684 - val_acc: 0.4126\n",
            "Epoch 4/10\n",
            "554/554 [==============================] - 90s 162ms/step - loss: 0.6014 - acc: 0.6806 - val_loss: 0.8043 - val_acc: 0.4090\n",
            "Epoch 5/10\n",
            "554/554 [==============================] - 87s 156ms/step - loss: 0.5933 - acc: 0.6875 - val_loss: 0.9619 - val_acc: 0.3743\n",
            "Epoch 6/10\n",
            "554/554 [==============================] - 90s 162ms/step - loss: 0.5877 - acc: 0.6954 - val_loss: 0.8277 - val_acc: 0.4176\n",
            "Epoch 7/10\n",
            "554/554 [==============================] - 87s 157ms/step - loss: 0.5827 - acc: 0.6956 - val_loss: 0.8084 - val_acc: 0.4357\n",
            "Epoch 8/10\n",
            "554/554 [==============================] - 90s 162ms/step - loss: 0.5761 - acc: 0.7001 - val_loss: 0.7448 - val_acc: 0.4855\n",
            "Epoch 9/10\n",
            "554/554 [==============================] - 87s 157ms/step - loss: 0.5712 - acc: 0.7072 - val_loss: 0.8500 - val_acc: 0.4220\n",
            "Epoch 10/10\n",
            "554/554 [==============================] - 93s 168ms/step - loss: 0.5681 - acc: 0.7103 - val_loss: 0.8673 - val_acc: 0.4162\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from aimodelshare.aimsonnx import model_to_onnx\n",
        "\n",
        "onnx_model = model_to_onnx(model_5, framework='keras',\n",
        "                          transfer_learning=False,\n",
        "                          deep_learning=True)\n",
        "\n",
        "with open(\"model_5.onnx\", \"wb\") as f:\n",
        "    f.write(onnx_model.SerializeToString())"
      ],
      "metadata": {
        "id": "Qlhkd2WNJJ__"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prediction_column_index=model_5.predict(preprocessor(X_test)).argmax(axis=1)\n",
        "prediction_labels = [y_train.columns[i] for i in prediction_column_index]\n",
        "mycompetition.submit_model(model_filepath = \"model_5.onnx\",\n",
        "                                 preprocessor_filepath=\"preprocessor.zip\",\n",
        "                                 prediction_submission=prediction_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YVIEqhqzJNLF",
        "outputId": "f0adf2cd-affa-4e51-88b1-d4bab1619b38"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "57/57 [==============================] - 7s 96ms/step\n",
            "Insert search tags to help users find your model (optional): \n",
            "Provide any useful notes about your model (optional): \n",
            "\n",
            "Your model has been submitted as model version 359\n",
            "\n",
            "To submit code used to create this model or to view current leaderboard navigate to Model Playground: \n",
            "\n",
            " https://www.modelshare.org/detail/model:2763\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Embedding, LSTM, Flatten\n",
        "\n",
        "model_6 = Sequential()\n",
        "model_6.add(Embedding(max_words, embedding_dim, input_length=maxlen))\n",
        "model_6.add(LSTM(64, return_sequences=True, dropout=0.2))\n",
        "model_6.add(LSTM(64, dropout=0.2))\n",
        "model_6.add(Flatten())\n",
        "model_6.add(Dense(2, activation='softmax'))\n",
        "\n",
        "model_6.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EwJu51GMJR1X",
        "outputId": "ea18d3d7-673d-4d9e-9dd3-e0290fac6a53"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_2 (Embedding)     (None, 100, 100)          2000000   \n",
            "                                                                 \n",
            " lstm_4 (LSTM)               (None, 100, 64)           42240     \n",
            "                                                                 \n",
            " lstm_5 (LSTM)               (None, 64)                33024     \n",
            "                                                                 \n",
            " flatten_2 (Flatten)         (None, 64)                0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 2)                 130       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,075,394\n",
            "Trainable params: 2,075,394\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_6.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
        "history = model_6.fit(preprocessor(X_train), y_train,\n",
        "                    epochs=10,\n",
        "                    batch_size=10,\n",
        "                    validation_split=0.4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8vQ_d7TwJTfV",
        "outputId": "0d9fe1af-6841-44eb-831c-40dc1481e7a2"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "416/416 [==============================] - 91s 208ms/step - loss: 0.5081 - acc: 0.7926 - val_loss: 1.6151 - val_acc: 0.1145\n",
            "Epoch 2/10\n",
            "416/416 [==============================] - 85s 205ms/step - loss: 0.4862 - acc: 0.7934 - val_loss: 1.2605 - val_acc: 0.1326\n",
            "Epoch 3/10\n",
            "416/416 [==============================] - 75s 179ms/step - loss: 0.4789 - acc: 0.7943 - val_loss: 1.5374 - val_acc: 0.1427\n",
            "Epoch 4/10\n",
            "416/416 [==============================] - 73s 177ms/step - loss: 0.4746 - acc: 0.8006 - val_loss: 1.5050 - val_acc: 0.1618\n",
            "Epoch 5/10\n",
            "416/416 [==============================] - 73s 177ms/step - loss: 0.4757 - acc: 0.8001 - val_loss: 1.9015 - val_acc: 0.1449\n",
            "Epoch 6/10\n",
            "416/416 [==============================] - 70s 169ms/step - loss: 0.4695 - acc: 0.8018 - val_loss: 1.6918 - val_acc: 0.1405\n",
            "Epoch 7/10\n",
            "416/416 [==============================] - 72s 173ms/step - loss: 0.4660 - acc: 0.8056 - val_loss: 1.5187 - val_acc: 0.1832\n",
            "Epoch 8/10\n",
            "416/416 [==============================] - 73s 175ms/step - loss: 0.4600 - acc: 0.8064 - val_loss: 1.1711 - val_acc: 0.1767\n",
            "Epoch 9/10\n",
            "416/416 [==============================] - 79s 189ms/step - loss: 0.4587 - acc: 0.8035 - val_loss: 1.2606 - val_acc: 0.1669\n",
            "Epoch 10/10\n",
            "416/416 [==============================] - 71s 170ms/step - loss: 0.4565 - acc: 0.8090 - val_loss: 1.3558 - val_acc: 0.2139\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from aimodelshare.aimsonnx import model_to_onnx\n",
        "\n",
        "onnx_model = model_to_onnx(model_6, framework='keras',\n",
        "                          transfer_learning=False,\n",
        "                          deep_learning=True)\n",
        "\n",
        "with open(\"model_6.onnx\", \"wb\") as f:\n",
        "    f.write(onnx_model.SerializeToString())"
      ],
      "metadata": {
        "id": "9ha8qTeoJVO6"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prediction_column_index=model_6.predict(preprocessor(X_test)).argmax(axis=1)\n",
        "prediction_labels = [y_train.columns[i] for i in prediction_column_index]\n",
        "mycompetition.submit_model(model_filepath = \"model_6.onnx\",\n",
        "                                 preprocessor_filepath=\"preprocessor.zip\",\n",
        "                                 prediction_submission=prediction_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yU4JsxxlJWtP",
        "outputId": "f6a2f899-c1dc-49b5-bbf5-82658296f3e2"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "57/57 [==============================] - 3s 38ms/step\n",
            "Insert search tags to help users find your model (optional): \n",
            "Provide any useful notes about your model (optional): \n",
            "\n",
            "Your model has been submitted as model version 363\n",
            "\n",
            "To submit code used to create this model or to view current leaderboard navigate to Model Playground: \n",
            "\n",
            " https://www.modelshare.org/detail/model:2763\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Embedding, LSTM, Flatten\n",
        "\n",
        "model_7 = Sequential()\n",
        "model_7.add(Embedding(max_words, embedding_dim, input_length=maxlen))\n",
        "model_7.add(LSTM(128, return_sequences=True, dropout=0.2))\n",
        "model_7.add(LSTM(128, dropout=0.2))\n",
        "model_7.add(Flatten())\n",
        "model_7.add(Dense(2, activation='softmax'))\n",
        "\n",
        "model_7.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PwJtwonMYUZC",
        "outputId": "4674c0ab-9c56-44b5-d719-f1b72cd2ccce"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_3 (Embedding)     (None, 100, 100)          2000000   \n",
            "                                                                 \n",
            " lstm_6 (LSTM)               (None, 100, 128)          117248    \n",
            "                                                                 \n",
            " lstm_7 (LSTM)               (None, 128)               131584    \n",
            "                                                                 \n",
            " flatten_3 (Flatten)         (None, 128)               0         \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 2)                 258       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,249,090\n",
            "Trainable params: 2,249,090\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_7.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
        "history = model_7.fit(preprocessor(X_train), y_train,\n",
        "                    epochs=10,\n",
        "                    batch_size=10,\n",
        "                    validation_split=0.4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uFOYnGEOYa--",
        "outputId": "a2da4256-fe98-471e-ceff-c00bfb39f26f"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "416/416 [==============================] - 150s 349ms/step - loss: 0.5101 - acc: 0.7934 - val_loss: 2.0470 - val_acc: 0.1149\n",
            "Epoch 2/10\n",
            "416/416 [==============================] - 135s 326ms/step - loss: 0.4894 - acc: 0.7921 - val_loss: 1.4565 - val_acc: 0.1236\n",
            "Epoch 3/10\n",
            "416/416 [==============================] - 136s 326ms/step - loss: 0.4810 - acc: 0.7972 - val_loss: 2.4488 - val_acc: 0.1171\n",
            "Epoch 4/10\n",
            "416/416 [==============================] - 136s 328ms/step - loss: 0.4784 - acc: 0.7984 - val_loss: 1.8758 - val_acc: 0.1702\n",
            "Epoch 5/10\n",
            "416/416 [==============================] - 136s 326ms/step - loss: 0.4771 - acc: 0.8044 - val_loss: 1.3658 - val_acc: 0.1600\n",
            "Epoch 6/10\n",
            "416/416 [==============================] - 136s 326ms/step - loss: 0.4724 - acc: 0.8035 - val_loss: 1.2796 - val_acc: 0.1842\n",
            "Epoch 7/10\n",
            "416/416 [==============================] - 135s 325ms/step - loss: 0.4647 - acc: 0.8061 - val_loss: 1.3551 - val_acc: 0.1980\n",
            "Epoch 8/10\n",
            "416/416 [==============================] - 135s 325ms/step - loss: 0.4613 - acc: 0.8102 - val_loss: 1.4834 - val_acc: 0.2157\n",
            "Epoch 9/10\n",
            "416/416 [==============================] - 135s 326ms/step - loss: 0.4563 - acc: 0.8131 - val_loss: 1.3363 - val_acc: 0.2276\n",
            "Epoch 10/10\n",
            "416/416 [==============================] - 135s 325ms/step - loss: 0.4539 - acc: 0.8088 - val_loss: 1.4717 - val_acc: 0.1799\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from aimodelshare.aimsonnx import model_to_onnx\n",
        "\n",
        "onnx_model = model_to_onnx(model_7, framework='keras',\n",
        "                          transfer_learning=False,\n",
        "                          deep_learning=True)\n",
        "\n",
        "with open(\"model_7.onnx\", \"wb\") as f:\n",
        "    f.write(onnx_model.SerializeToString())"
      ],
      "metadata": {
        "id": "wNMM560CYhR9"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prediction_column_index=model_7.predict(preprocessor(X_test)).argmax(axis=1)\n",
        "prediction_labels = [y_train.columns[i] for i in prediction_column_index]\n",
        "mycompetition.submit_model(model_filepath = \"model_7.onnx\",\n",
        "                                 preprocessor_filepath=\"preprocessor.zip\",\n",
        "                                 prediction_submission=prediction_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3o5SSMu_Yjt-",
        "outputId": "2c0d066d-5587-4c4d-a939-7fc52ec75942"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "57/57 [==============================] - 15s 247ms/step\n",
            "Insert search tags to help users find your model (optional): \n",
            "Provide any useful notes about your model (optional): \n",
            "\n",
            "Your model has been submitted as model version 375\n",
            "\n",
            "To submit code used to create this model or to view current leaderboard navigate to Model Playground: \n",
            "\n",
            " https://www.modelshare.org/detail/model:2763\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Discuss which models you tried and which models performed better and point out relevant hyper-parameter values for successful models.**"
      ],
      "metadata": {
        "id": "wrACHImTmFX9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "My teammates also saw higher model performance from models that use LSTM layers. They used a bigger number for epoch, so I tried using 10 as epoch to see the change in performance. Comparing Model 4 and 5, which have different size filters, they ended up having the same accuracy of 0.7103. This indicates that epoch might have more value in altering model performance than filter sizes. For Model 6, I tried using a higher number of 0.4 (vs. 0.2) for validation data split, and all else equal as Model 5. In fact, the change in validation set has given significantly higher accuracy with 0.8090 at epoch 10/10. Even at 1/10 epoch, this model had accuracy of 0.7926, which is higher than the previous models. Lastly, I tried Model 7 with 0.4 validation data split with larger filter size. The accuracy did not improve, which indicate that filter sizes do not impact the performance much in the LSTM models."
      ],
      "metadata": {
        "id": "4HAqySAdT-F0"
      }
    }
  ]
}
